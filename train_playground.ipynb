{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11b206370>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch.utils.data import random_split\n",
    "import torch\n",
    "from torch import Tensor, LongTensor\n",
    "from torch_geometric.nn import radius_graph\n",
    "import torch.nn as nn\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import TransformerConv\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import Data\n",
    "from typing import Tuple\n",
    "from ase import Atoms\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cbp.model import CBPSimpleMLP\n",
    "model = CBPSimpleMLP(num_features=3696, num_hidden=1000, num_targets=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.5)\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(train_loader))\n",
    "def train():\n",
    "    model.train()\n",
    "    # get first batch only\n",
    "    optimizer.zero_grad()\n",
    "    # get node embeddings\n",
    "    node_embeddings = model(data)\n",
    "    # get graph embedding\n",
    "    out = global_mean_pool(node_embeddings, data.batch)\n",
    "    loss = criterion(out[:,7], data.y[:,7])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Module [MLP] is missing the required \"forward\" function",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m500\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[43], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# get node embeddings\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m node_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# get graph embedding\u001b[39;00m\n\u001b[1;32m      9\u001b[0m out \u001b[38;5;241m=\u001b[39m global_mean_pool(node_embeddings, data\u001b[38;5;241m.\u001b[39mbatch)\n",
      "File \u001b[0;32m~/software/ngpt-mlip/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/software/ngpt-mlip/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/software/ngpt-mlip/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:394\u001b[0m, in \u001b[0;36m_forward_unimplemented\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_unimplemented\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Define the computation performed at every call.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Should be overridden by all subclasses.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;124;03m        registered hooks while the latter silently ignores them.\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 394\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    395\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] is missing the required \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m function\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    396\u001b[0m     )\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Module [MLP] is missing the required \"forward\" function"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 500):\n",
    "    loss = train()\n",
    "    scheduler.step()\n",
    "    if epoch % 50 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss:.4f}, LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = QM9(root='./data')\n",
    "\n",
    "y_values = dataset.y\n",
    "\n",
    "# normalize y values along each column\n",
    "y_values = (y_values - y_values.mean(dim=0)) / y_values.std(dim=0)\n",
    "dataset._data.y = y_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dscribe.descriptors import SOAP\n",
    "soap_desc = SOAP(species=[\"C\", \"H\", \"O\", \"N\", \"F\"], r_cut=5, n_max=8, l_max=6, average=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01623526, 0.06291466, 0.15724008, ..., 0.        , 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ase import Atoms\n",
    "atoms = Atoms(numbers=dataset[0].z.tolist(), positions=dataset[0].pos.tolist())\n",
    "desc = soap_desc.create(atoms)\n",
    "desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "setattr(dataset[0], 'soap', desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[5, 11], edge_index=[2, 8], edge_attr=[8, 4], y=[1, 19], pos=[5, 3], idx=[1], name='gdb_1', z=[5])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator().manual_seed(42)\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [0.85, 0.05, 0.1], generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dataset, 'data/splits/dataset.pt')\n",
    "torch.save(train_dataset, 'data/splits/train_dataset.pt')\n",
    "torch.save(val_dataset, 'data/splits/val_dataset.pt')\n",
    "torch.save(test_dataset, 'data/splits/test_dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in dataste\n",
    "train_dataset = torch.load('data/splits/train_dataset.pt', weights_only=False)\n",
    "val_dataset = torch.load('data/splits/val_dataset.pt', weights_only=False)\n",
    "test_dataset = torch.load('data/splits/test_dataset.pt', weights_only=False)\n",
    "# create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablo/software/ngpt-mlip/.venv/lib/python3.11/site-packages/torch_geometric/io/fs.py:229: UserWarning: Weights only load failed. Please file an issue to make `torch.load(weights_only=True)` compatible in your case. Please use `torch.serialization.add_safe_globals([_reconstruct])` to allowlist this global.\n",
      "  warnings.warn(f\"{warn_msg} Please use \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(x=[5, 11], edge_index=[2, 8], edge_attr=[8, 4], y=[1, 19], pos=[5, 3], idx=[1], name='gdb_1', z=[5], soap=[5740])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AddFeatureTransform:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def __call__(self, data):\n",
    "        # Calculate your new feature here\n",
    "        data.soap = self.calc_soap(data)\n",
    "        return data\n",
    "    \n",
    "    def calc_soap(self, data):\n",
    "        atoms = Atoms(numbers=data.z.tolist(), positions=data.pos.tolist())\n",
    "        desc = soap_desc.create(atoms)\n",
    "        return desc\n",
    "\n",
    "# Usage\n",
    "transform = AddFeatureTransform()\n",
    "dataset = QM9(root=\"data\", force_reload=False, pre_transform=transform)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator().manual_seed(42)\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [0.85, 0.05, 0.1], generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# torch.save(dataset, 'data/splits/dataset.pt')\n",
    "# torch.save(train_dataset, 'data/splits/train_dataset.pt')\n",
    "# torch.save(val_dataset, 'data/splits/val_dataset.pt')\n",
    "torch.save(test_dataset, 'data/splits/test_dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianSmearing(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        start: float = 0.0,\n",
    "        stop: float = 5.0,\n",
    "        num_gaussians: int = 50,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        offset = torch.linspace(start, stop, num_gaussians)\n",
    "        self.coeff = -0.5 / (offset[1] - offset[0]).item()**2\n",
    "        self.register_buffer('offset', offset)\n",
    "\n",
    "    def forward(self, dist: Tensor) -> Tensor:\n",
    "        dist = dist.view(-1, 1) - self.offset.view(1, -1)\n",
    "        return torch.exp(self.coeff * torch.pow(dist, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadiusInteractionGraph(torch.nn.Module):\n",
    "    r\"\"\"Creates edges based on atom positions :obj:`pos` to all points within\n",
    "    the cutoff distance.\n",
    "\n",
    "    Args:\n",
    "        cutoff (float, optional): Cutoff distance for interatomic interactions.\n",
    "            (default: :obj:`10.0`)\n",
    "        max_num_neighbors (int, optional): The maximum number of neighbors to\n",
    "            collect for each node within the :attr:`cutoff` distance with the\n",
    "            default interaction graph method.\n",
    "            (default: :obj:`32`)\n",
    "    \"\"\"\n",
    "    def __init__(self, cutoff: float = 10.0, max_num_neighbors: int = 32):\n",
    "        super().__init__()\n",
    "        self.cutoff = cutoff\n",
    "        self.max_num_neighbors = max_num_neighbors\n",
    "\n",
    "    def forward(self, pos: Tensor, batch: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            pos (Tensor): Coordinates of each atom.\n",
    "            batch (LongTensor, optional): Batch indices assigning each atom to\n",
    "                a separate molecule.\n",
    "\n",
    "        :rtype: (:class:`LongTensor`, :class:`Tensor`)\n",
    "        \"\"\"\n",
    "        edge_index = radius_graph(pos, r=self.cutoff, batch=batch,\n",
    "                                  max_num_neighbors=self.max_num_neighbors)\n",
    "        row, col = edge_index\n",
    "        edge_weight = (pos[row] - pos[col]).norm(dim=-1)\n",
    "        return edge_index, edge_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, num_features, num_targets, heads, num_gaussians):\n",
    "        super().__init__()\n",
    "        # GATConv with edge features\n",
    "        self.conv = GATConv(\n",
    "            in_channels=num_features, \n",
    "            out_channels=num_targets,\n",
    "            heads=heads,\n",
    "            edge_dim=num_gaussians  # match your num_gaussians for edge attributes\n",
    "        )\n",
    "        self.ln = nn.LayerNorm(num_targets * heads)\n",
    "        self.ff = nn.Linear(num_targets * heads, num_features)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight, edge_attr):\n",
    "        # GATConv only needs x, edge_index, and edge_attr\n",
    "        out = self.conv(x, edge_index, edge_attr=edge_attr)\n",
    "        out = self.ln(out)\n",
    "        out = self.ff(out)\n",
    "        return x + out  # residual connection\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_channels: int = 128,\n",
    "        num_features: int = 11,\n",
    "        num_targets: int = 19,\n",
    "        heads: int = 8,\n",
    "        cutoff: float = 5.0,\n",
    "        max_num_neighbors: int = 32,\n",
    "        num_gaussians: int = 50,\n",
    "        num_blocks: int = 3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(100, hidden_channels, padding_idx=0)\n",
    "        self.interaction_graph = RadiusInteractionGraph(cutoff, max_num_neighbors)\n",
    "        self.distance_expansion = GaussianSmearing(0.0, cutoff, num_gaussians)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                AttentionBlock(hidden_channels, num_targets, heads, num_gaussians)\n",
    "                for _ in range(num_blocks)\n",
    "            ]\n",
    "        )\n",
    "        self.lin1 = nn.Linear(hidden_channels, hidden_channels // 2)\n",
    "        self.act = nn.ReLU()\n",
    "        self.lin2 = nn.Linear(hidden_channels // 2, num_targets)\n",
    "\n",
    "    def forward(self, data):\n",
    "        h = self.embedding(data.z)\n",
    "        edge_index, edge_weight = self.interaction_graph(data.pos, data.batch)\n",
    "        edge_attr = self.distance_expansion(edge_weight)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            h = h + block(h, edge_index, edge_weight, edge_attr)\n",
    "\n",
    "        out = self.lin1(h)\n",
    "        out = self.act(out)\n",
    "        out = self.lin2(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.5)\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on one batch to test the model\n",
    "data = next(iter(train_loader))\n",
    "def train():\n",
    "    model.train()\n",
    "    # get first batch only\n",
    "    optimizer.zero_grad()\n",
    "    # get node embeddings\n",
    "    node_embeddings = model(data)\n",
    "    # get graph embedding\n",
    "    out = global_mean_pool(node_embeddings, data.batch)\n",
    "    loss = criterion(out[:,7], data.y[:,7])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Loss: 0.2424, LR: 0.001000\n",
      "Epoch 100, Loss: 0.1470, LR: 0.001000\n",
      "Epoch 150, Loss: 0.0455, LR: 0.001000\n",
      "Epoch 200, Loss: 0.0246, LR: 0.001000\n",
      "Epoch 250, Loss: 0.0086, LR: 0.001000\n",
      "Epoch 300, Loss: 0.0089, LR: 0.001000\n",
      "Epoch 350, Loss: 0.0013, LR: 0.001000\n",
      "Epoch 400, Loss: 0.0009, LR: 0.001000\n",
      "Epoch 450, Loss: 0.0003, LR: 0.001000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 500):\n",
    "    loss = train()\n",
    "    scheduler.step()\n",
    "    if epoch % 50 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss:.4f}, LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1652, -1.2174, -0.3294,  0.7120, -1.0163, -3.9658, -0.2178, -1.2608,\n",
      "         0.2047, -0.1852,  1.5539, -0.2822, -2.9801,  0.6996, -0.7121,  0.2769,\n",
      "        -0.6954,  0.7406, -0.1978,  0.5902, -0.2861,  0.7648,  1.5744, -0.6649,\n",
      "        -0.2589,  2.4867, -1.0844, -0.3288,  0.1834, -0.4734, -1.1804, -0.1702,\n",
      "         0.3361,  0.3196,  4.0020,  1.1726, -0.7010, -1.2437,  1.1501, -0.2748,\n",
      "        -0.6873,  0.2087, -0.3241, -1.6299, -1.6005,  0.6289,  0.1907,  0.2356,\n",
      "        -0.9825,  1.3913,  0.6752, -1.6797,  1.5454,  1.5256, -1.1322, -0.1567,\n",
      "         0.6679, -0.6931,  0.6295, -1.2356, -1.5883, -0.1738, -1.0776, -1.5624])\n",
      "tensor([-0.1814, -1.2356, -0.3352,  0.6782, -1.0160, -3.9753, -0.2403, -1.2358,\n",
      "         0.1932, -0.2100,  1.5271, -0.3054, -2.9874,  0.6894, -0.7376,  0.2757,\n",
      "        -0.7062,  0.7238, -0.2098,  0.5653, -0.3051,  0.7429,  1.5504, -0.6802,\n",
      "        -0.2748,  2.4801, -1.1115, -0.3668,  0.1627, -0.4872, -1.2069, -0.1791,\n",
      "         0.3177,  0.3059,  3.9877,  1.1566, -0.7082, -1.2371,  1.1252, -0.3061,\n",
      "        -0.7090,  0.1931, -0.3360, -1.6382, -1.6090,  0.6184,  0.1626,  0.2221,\n",
      "        -0.9868,  1.4353,  0.6572, -1.6848,  1.4953,  1.4354, -1.1412, -0.1795,\n",
      "         0.6563, -0.7072,  0.6249, -1.2670, -1.6079, -0.1795, -1.0827, -1.5778])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(data)\n",
    "    out = global_mean_pool(out, data.batch)\n",
    "    print(out[:,7])\n",
    "    print(data.y[:,7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(next(iter(test_loader)))\n",
    "    print(out[:,7].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(test_loader))\n",
    "print(data.y[:,7].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 19])\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    total_loss = 0  \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            out = model(data.x, data.edge_index)\n",
    "            out = global_mean_pool(out, data.batch)\n",
    "            loss = criterion(out, data.y)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
